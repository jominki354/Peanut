import json
import os
import argparse
import subprocess
import sys
from pathlib import Path

def check_requirements():
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        import llama_cpp
        print("âœ… llama-cpp-pythonì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    except ImportError:
        print("âŒ llama-cpp-pythonì´ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install llama-cpp-python")
        return False
    
    return True

def load_dataset(jsonl_path):
    """JSONL íŒŒì¼ì—ì„œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    dataset = []
    try:
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line.strip())
                dataset.append(data)
        print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset)}ê°œ QA ìŒ")
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        if dataset:
            print("\në°ì´í„°ì…‹ ìƒ˜í”Œ:")
            print(f"ì§ˆë¬¸: {dataset[0]['prompt'][:100]}...")
            print(f"ë‹µë³€: {dataset[0]['response'][:100]}...")
        
        return dataset
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None

def prepare_finetune_format(dataset, output_path, model_type="llama"):
    """íŒŒì¸íŠœë‹ì„ ìœ„í•œ í¬ë§·ìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ë³€í™˜í•©ë‹ˆë‹¤."""
    formatted_data = []
    
    if model_type.lower() == "llama":
        # Llama ëª¨ë¸ì„ ìœ„í•œ í¬ë§·
        for item in dataset:
            formatted_data.append({
                "text": f"<s>[INST] {item['prompt']} [/INST] {item['response']}</s>"
            })
    elif model_type.lower() == "mistral":
        # Mistral ëª¨ë¸ì„ ìœ„í•œ í¬ë§·
        for item in dataset:
            formatted_data.append({
                "text": f"<s>[INST] {item['prompt']} [/INST] {item['response']}</s>"
            })
    elif model_type.lower() == "gemma":
        # Gemma ëª¨ë¸ì„ ìœ„í•œ í¬ë§·
        for item in dataset:
            formatted_data.append({
                "text": f"<start_of_turn>user\n{item['prompt']}<end_of_turn>\n<start_of_turn>model\n{item['response']}<end_of_turn>"
            })
    else:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
        return False
    
    # í¬ë§·ëœ ë°ì´í„° ì €ì¥
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in formatted_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        print(f"âœ… í¬ë§· ë³€í™˜ ì™„ë£Œ. íŒŒì¼ ì €ì¥: {output_path}")
        return True
    except Exception as e:
        print(f"âŒ í¬ë§· ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
        return False

def run_finetune(model_path, dataset_path, output_dir, epochs=3, ctx_size=2048, model_type="llama"):
    """llama.cppë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¸íŠœë‹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ íŒŒì¸íŠœë‹ ëª…ë ¹ì–´ ìƒì„±
        if model_type.lower() in ["llama", "mistral", "gemma"]:
            # llama.cppì˜ finetune ëª…ë ¹ì–´
            cmd = [
                "llama-finetune",
                "--model", model_path,
                "--lora-out", os.path.join(output_dir, "carrotpilot-lora.bin"),
                "--train-data", dataset_path,
                "--epochs", str(epochs),
                "--ctx-size", str(ctx_size),
                "--threads", str(max(1, os.cpu_count() // 2)),  # CPU ì½”ì–´ì˜ ì ˆë°˜ ì‚¬ìš©
                "--learning-rate", "5e-5",
                "--lora-r", "8",
                "--adam-beta1", "0.9",
                "--adam-beta2", "0.999",
                "--adam-eps", "1e-8",
                "--batch-size", "8",  # ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
                "--checkpoint-out", os.path.join(output_dir, "checkpoint"),
                "--checkpoint-steps", "50"
            ]
            
            print("ğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘...")
            print(f"ëª…ë ¹ì–´: {' '.join(cmd)}")
            
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
            
            # ì‹¤ì‹œê°„ ì¶œë ¥ í‘œì‹œ
            for line in process.stdout:
                print(line, end='')
            
            process.wait()
            
            if process.returncode == 0:
                print(f"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! LoRA ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {os.path.join(output_dir, 'carrotpilot-lora.bin')}")
                return True
            else:
                print(f"âŒ íŒŒì¸íŠœë‹ ì‹¤íŒ¨: ì¢…ë£Œ ì½”ë“œ {process.returncode}")
                return False
        else:
            print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
            return False
    except Exception as e:
        print(f"âŒ íŒŒì¸íŠœë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

def main():
    parser = argparse.ArgumentParser(description="ë¡œì»¬ LLMì— ë‹¹ê·¼íŒŒì¼ëŸ¿ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹")
    parser.add_argument("--model", required=True, help="GGUF íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--model-type", default="llama", choices=["llama", "mistral", "gemma"], 
                        help="ëª¨ë¸ ìœ í˜• (llama, mistral, gemma)")
    parser.add_argument("--dataset", default="carrotpilot_data_with_images/carrotpilot_finetuning_dataset.jsonl", 
                        help="JSONL ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output-dir", default="carrotpilot_finetuned", help="íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬")
    parser.add_argument("--epochs", type=int, default=3, help="ì—í¬í¬ ìˆ˜")
    parser.add_argument("--ctx-size", type=int, default=2048, help="ì»¨í…ìŠ¤íŠ¸ í¬ê¸°")
    
    args = parser.parse_args()
    
    # ìš”êµ¬ì‚¬í•­ ì²´í¬
    if not check_requirements():
        sys.exit(1)
    
    # ëª¨ë¸ íŒŒì¼ ì²´í¬
    if not os.path.exists(args.model):
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.model}")
        sys.exit(1)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = load_dataset(args.dataset)
    if not dataset:
        sys.exit(1)
    
    # íŒŒì¸íŠœë‹ í¬ë§·ìœ¼ë¡œ ë³€í™˜
    formatted_dataset_path = os.path.join(Path(args.output_dir), "formatted_dataset.jsonl")
    if not prepare_finetune_format(dataset, formatted_dataset_path, args.model_type):
        sys.exit(1)
    
    # íŒŒì¸íŠœë‹ ì‹¤í–‰
    if not run_finetune(args.model, formatted_dataset_path, args.output_dir, args.epochs, args.ctx_size, args.model_type):
        sys.exit(1)
    
    print("âœ¨ ëª¨ë“  ê³¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ğŸ” LoRA ëª¨ë¸ ìœ„ì¹˜: {os.path.join(args.output_dir, 'carrotpilot-lora.bin')}")
    print(f"""
íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ:
llama-cpp-pythonì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°:
from llama_cpp import Llama
model = Llama(
    model_path="{args.model}",
    lora_path="{os.path.join(args.output_dir, 'carrotpilot-lora.bin')}",
    n_ctx=2048,
    n_gpu_layers=-1  # ê°€ëŠ¥í•œ ëª¨ë“  ë ˆì´ì–´ì— GPU ì‚¬ìš©
)
output = model.create_completion("ë‹¹ê·¼íŒŒì¼ëŸ¿ì— ëŒ€í•´ ì•Œë ¤ì¤˜", max_tokens=1024)
print(output["choices"][0]["text"])
""")

if __name__ == "__main__":
    main() 