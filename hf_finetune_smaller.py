import os
import json
import torch
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training
import pandas as pd

def main():
    # ì„¤ì •
    output_dir = "carrotpilot_finetuned_gemma3"
    dataset_path = "carrotpilot_data_with_images/carrotpilot_finetuning_dataset.jsonl"
    model_name = "google/gemma-3-2b-instruct"  # ë” ì‘ì€ Gemma 3 2B ëª¨ë¸ ì‚¬ìš©
    
    print(f"ğŸ” ë°ì´í„°ì…‹: {dataset_path}")
    print(f"ğŸ” ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"ğŸ” ëª¨ë¸: {model_name}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    print("ğŸ“š ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = []
    with open(dataset_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line.strip())
            dataset.append(data)
    
    print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset)}ê°œ QA ìŒ")
    
    # ë°ì´í„°ì…‹ í¬ë§·íŒ… (Gemma 3 í˜•ì‹ìœ¼ë¡œ)
    print("ğŸ”„ ë°ì´í„°ì…‹ í¬ë§·íŒ… ì¤‘...")
    formatted_data = []
    for item in dataset:
        formatted_data.append({
            "text": f"<start_of_turn>user\n{item['prompt']}<end_of_turn>\n<start_of_turn>model\n{item['response']}<end_of_turn>"
        })
    
    # HF Datasetìœ¼ë¡œ ë³€í™˜
    print("ğŸ”„ HF Dataset ë³€í™˜ ì¤‘...")
    df = pd.DataFrame(formatted_data)
    hf_dataset = Dataset.from_pandas(df)
    print(f"âœ… HF Dataset ë³€í™˜ ì™„ë£Œ: {len(hf_dataset)}ê°œ í•­ëª©")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"ğŸ”„ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ë°ì´í„°ì…‹ í† í°í™”
    print("ğŸ”„ ë°ì´í„°ì…‹ í† í°í™” ì¤‘...")
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=256,  # ì»¨í…ìŠ¤íŠ¸ í¬ê¸° ë” ì¶•ì†Œ (ë©”ëª¨ë¦¬ ì ˆì•½)
            return_tensors="pt"
        )
    
    tokenized_dataset = hf_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=["text"]
    )
    print(f"âœ… ë°ì´í„°ì…‹ í† í°í™” ì™„ë£Œ")
    
    # í•™ìŠµ/ê²€ì¦ ë¶„í• 
    split_dataset = tokenized_dataset.train_test_split(test_size=0.1)
    
    # 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
    print("ğŸ”„ 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì • ì¤‘...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )
    
    # ëª¨ë¸ ë¡œë“œ
    print(f"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        # LoRA í•™ìŠµì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„
        print("ğŸ”„ LoRA í•™ìŠµì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
        model = prepare_model_for_kbit_training(model)
        
        # LoRA ì„¤ì • - GTX 1660ì— ë§ê²Œ ë§¤ê°œë³€ìˆ˜ ì¶•ì†Œ
        peft_config = LoraConfig(
            task_type="CAUSAL_LM",
            r=2,  # ìˆœìœ„ ë” ê°ì†Œ (ë§¤ìš° ì‘ê²Œ)
            lora_alpha=8,
            lora_dropout=0.05,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            bias="none",
        )
        
        # LoRA ì–´ëŒ‘í„° ì ìš©
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
        
        # í•™ìŠµ ì¸ì ì„¤ì • - GTX 1660ì— ë§ê²Œ ì¡°ì •
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=1,  # ì—í¬í¬ ìˆ˜ ê°ì†Œ
            per_device_train_batch_size=2,  # ì‘ì€ ëª¨ë¸ì´ë¼ ë°°ì¹˜ í¬ê¸° 2
            per_device_eval_batch_size=2,
            gradient_accumulation_steps=8,  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„
            evaluation_strategy="steps",
            eval_steps=20,
            save_strategy="steps",
            save_steps=20,
            save_total_limit=2,
            logging_dir=os.path.join(output_dir, "logs"),
            logging_steps=10,
            learning_rate=5e-5,
            weight_decay=0.01,
            fp16=True,
            bf16=False,
            max_grad_norm=0.3,
            warmup_ratio=0.03,
            group_by_length=True,
            lr_scheduler_type="cosine",
            report_to="none",
            optim="paged_adamw_8bit",  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ 8ë¹„íŠ¸ ì˜µí‹°ë§ˆì´ì €
            gradient_checkpointing=True  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
        )
        
        # ë°ì´í„° ì½œë ˆì´í„° ì •ì˜
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer, 
            mlm=False
        )
        
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=split_dataset["train"],
            eval_dataset=split_dataset["test"],
            data_collator=data_collator,
            tokenizer=tokenizer
        )
        
        # ëª¨ë¸ í•™ìŠµ
        print("ğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘...")
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        print(f"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ ì¤‘...")
        model.save_pretrained(os.path.join(output_dir, "final_model"))
        tokenizer.save_pretrained(os.path.join(output_dir, "final_model"))
        
        print(f"""
âœ¨ ëª¨ë“  ê³¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!
ğŸ” íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ìœ„ì¹˜: {os.path.join(output_dir, 'final_model')}

íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("{os.path.join(output_dir, 'final_model')}")
model = AutoModelForCausalLM.from_pretrained("{os.path.join(output_dir, 'final_model')}")

# í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ìƒì„±
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device=0)  # device=0ì€ ì²« ë²ˆì§¸ GPU ì‚¬ìš©

# ì¶”ë¡  ìˆ˜í–‰
prompt = "ë‹¹ê·¼íŒŒì¼ëŸ¿ì— ëŒ€í•´ ì•Œë ¤ì¤˜"
result = pipe(prompt, max_length=512, do_sample=True, temperature=0.7)
print(result[0]['generated_text'])
```

LMStudioì—ì„œ ì‚¬ìš©:
1. LMStudioì˜ 'ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°' ê¸°ëŠ¥ ì‚¬ìš©
2. '{os.path.join(output_dir, 'final_model')}' ê²½ë¡œë¥¼ ì§€ì •

âœ… ëª¨ë¸ì„ LMStudioì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´ Hugging Face ëª¨ë¸ í˜•ì‹ì„ GGUFë¡œ ë³€í™˜í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
""")
    except Exception as e:
        print(f"âŒ íŒŒì¸íŠœë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print("ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •ì„ ë” ì¡°ì •í•´ë³´ì„¸ìš”:")
        print("1. ë°°ì¹˜ í¬ê¸°ë¥¼ 1ë¡œ ì¤„ì´ê¸°")
        print("2. ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„ë¥¼ ëŠ˜ë¦¬ê¸°")
        print("3. ì»¨í…ìŠ¤íŠ¸ í¬ê¸°ë¥¼ ë” ì¤„ì´ê¸° (128)")
        print("4. LoRA ë­í¬(r)ë¥¼ 1ë¡œ ì„¤ì •")
        print("5. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš© (ì˜ˆ: Phi-1.5)")

if __name__ == "__main__":
    main() 