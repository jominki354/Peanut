import os
import json
import argparse
import torch
import subprocess
import shutil
from pathlib import Path
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training
import pandas as pd

def prepare_formatted_dataset(dataset_path, output_path, model_type="gemma"):
    """íŒŒì¸íŠœë‹ìš© í¬ë§· ë°ì´í„°ì…‹ ì¤€ë¹„"""
    print("ğŸ“š ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = []
    with open(dataset_path, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line.strip())
            dataset.append(data)
    
    print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset)}ê°œ QA ìŒ")
    
    # ë°ì´í„°ì…‹ í¬ë§·íŒ… (Gemma 3 í˜•ì‹ìœ¼ë¡œ)
    print("ğŸ”„ ë°ì´í„°ì…‹ í¬ë§·íŒ… ì¤‘...")
    formatted_data = []
    
    if model_type.lower() == "gemma":
        for item in dataset:
            formatted_data.append({
                "text": f"<start_of_turn>user\n{item['prompt']}<end_of_turn>\n<start_of_turn>model\n{item['response']}<end_of_turn>"
            })
    
    # í¬ë§·ëœ ë°ì´í„° ì €ì¥
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in formatted_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    print(f"âœ… í¬ë§· ë³€í™˜ ì™„ë£Œ. íŒŒì¼ ì €ì¥: {output_path}")
    return len(dataset)

def finetune_with_llama_cpp(model_path, dataset_path, output_dir, epochs=3, ctx_size=1024):
    """llama-cpp-pythonì„ ì‚¬ìš©í•˜ì—¬ íŒŒì¸íŠœë‹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # Hugging Faceì—ì„œ llama.cpp ê°€ì ¸ì˜¤ê¸°
        print("ğŸ”„ llama.cpp í´ë¡  ì¤‘...")
        if not os.path.exists("llama.cpp"):
            subprocess.run(["git", "clone", "https://github.com/ggerganov/llama.cpp"], check=True)
        
        # llama.cpp ë¹Œë“œ
        print("ğŸ”„ llama.cpp ë¹Œë“œ ì¤‘...")
        os.chdir("llama.cpp")
        subprocess.run(["cmake", "-B", "build"], check=True)
        subprocess.run(["cmake", "--build", "build", "--config", "Release"], check=True)
        
        # íŒŒì¸íŠœë‹ ëª…ë ¹ì–´ ìƒì„±
        finetune_cmd = [
            os.path.join("build", "bin", "Release", "finetune") if os.name == 'nt' else os.path.join("build", "bin", "finetune"),
            "--model", model_path,
            "--train-data", dataset_path,
            "--checkpoint-out", os.path.join("..", output_dir, "checkpoint"),
            "--checkpoint-steps", "50",
            "--lora-out", os.path.join("..", output_dir, "carrotpilot-lora.bin"),
            "--epochs", str(epochs),
            "--ctx-size", str(ctx_size),
            "--learning-rate", "5e-5",
            "--batch-size", "2",
            "--threads", "8",
            "--lora-r", "8"
        ]
        
        print("ğŸš€ llama.cppë¡œ íŒŒì¸íŠœë‹ ì‹œì‘...")
        print(f"ëª…ë ¹ì–´: {' '.join(finetune_cmd)}")
        
        process = subprocess.Popen(finetune_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
        
        # ì‹¤ì‹œê°„ ì¶œë ¥ í‘œì‹œ
        for line in process.stdout:
            print(line, end='')
        
        process.wait()
        
        # ì›ë˜ ë””ë ‰í† ë¦¬ë¡œ ëŒì•„ê°€ê¸°
        os.chdir("..")
        
        if process.returncode == 0:
            print(f"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! LoRA ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {os.path.join(output_dir, 'carrotpilot-lora.bin')}")
            return True
        else:
            print(f"âŒ íŒŒì¸íŠœë‹ ì‹¤íŒ¨: ì¢…ë£Œ ì½”ë“œ {process.returncode}")
            return False
    except Exception as e:
        print(f"âŒ íŒŒì¸íŠœë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        # ì›ë˜ ë””ë ‰í† ë¦¬ë¡œ ëŒì•„ê°€ê¸°
        if os.getcwd().endswith("llama.cpp"):
            os.chdir("..")
        return False

def clone_llama_cpp_if_needed():
    """llama.cppê°€ ì—†ìœ¼ë©´ í´ë¡ """
    if not os.path.exists("llama.cpp"):
        print("ğŸ”„ llama.cpp í´ë¡  ì¤‘...")
        try:
            subprocess.run(["git", "clone", "https://github.com/ggerganov/llama.cpp"], check=True)
            return True
        except Exception as e:
            print(f"âŒ llama.cpp í´ë¡  ì‹¤íŒ¨: {str(e)}")
            return False
    return True

def main():
    # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    model_path = "C:/Users/jomin/.lmstudio/models/lmstudio-community/gemma-3-4b-it-GGUF/gemma-3-4b-it-Q4_K_M.gguf"
    output_dir = "carrotpilot_finetuned_gemma3"
    dataset_path = "carrotpilot_data_with_images/carrotpilot_finetuning_dataset.jsonl"
    
    print(f"ğŸ” ë°ì´í„°ì…‹: {dataset_path}")
    print(f"ğŸ” ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print(f"ğŸ” ëª¨ë¸ ê²½ë¡œ: {model_path}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # í¬ë§·ëœ ë°ì´í„°ì…‹ ì¤€ë¹„
    formatted_dataset_path = os.path.join(output_dir, "formatted_dataset.jsonl")
    prepare_formatted_dataset(dataset_path, formatted_dataset_path, "gemma")
    
    # llama.cpp í´ë¡ 
    if not clone_llama_cpp_if_needed():
        print("llama.cppë¥¼ í´ë¡ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return
    
    # Visual Studio ìˆëŠ”ì§€ í™•ì¸
    try:
        result = subprocess.run(["cmake", "--version"], capture_output=True, text=True)
        if "cmake version" not in result.stdout:
            print("âŒ CMakeê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print("Visual Studio 2019 ë˜ëŠ” 2022ì™€ 'C++ë¥¼ ì‚¬ìš©í•œ ë°ìŠ¤í¬í†± ê°œë°œ' ì›Œí¬ë¡œë“œë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
            return
    except:
        print("âŒ CMakeê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        print("Visual Studio 2019 ë˜ëŠ” 2022ì™€ 'C++ë¥¼ ì‚¬ìš©í•œ ë°ìŠ¤í¬í†± ê°œë°œ' ì›Œí¬ë¡œë“œë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return
    
    # Hugging Face ëª¨ë¸ë¡œ íŒŒì¸íŠœë‹ ì‹œë„
    print("""
âš ï¸ GGUF íŒŒì¼ì„ ì§ì ‘ íŒŒì¸íŠœë‹í•˜ë ¤ë©´ llama.cppë¥¼ ë¹Œë“œí•´ì•¼ í•©ë‹ˆë‹¤.
   ì´ ê³¼ì •ì€ Visual Studio ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.
   
   ëŒ€ì•ˆìœ¼ë¡œ Hugging Face ëª¨ë¸ì„ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ?
   1. llama.cpp ë¹Œë“œ ì‹œë„ (Visual Studio í•„ìš”)
   2. ëŒ€ì‹  Hugging Faceì˜ ì›ë³¸ ëª¨ë¸ ì‚¬ìš©
    """)
    
    choice = input("ì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2): ")
    
    if choice == "1":
        # llama.cppë¡œ íŒŒì¸íŠœë‹
        result = finetune_with_llama_cpp(
            model_path, 
            formatted_dataset_path, 
            output_dir, 
            epochs=1, 
            ctx_size=1024
        )
        
        if result:
            print(f"""
âœ¨ íŒŒì¸íŠœë‹ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!
ğŸ” LoRA ëª¨ë¸ ìœ„ì¹˜: {os.path.join(output_dir, 'carrotpilot-lora.bin')}

LMStudioì—ì„œ ì‚¬ìš© ë°©ë²•:
1. ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {model_path}
2. LoRA ì–´ëŒ‘í„° ì¶”ê°€: {os.path.join(output_dir, 'carrotpilot-lora.bin')}

ë˜ëŠ” Pythonì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©:
```python
from llama_cpp import Llama

model = Llama(
    model_path="{model_path}",
    lora_path="{os.path.join(output_dir, 'carrotpilot-lora.bin')}",
    n_ctx=1024,
    n_gpu_layers=-1  # ê°€ëŠ¥í•œ ëª¨ë“  ë ˆì´ì–´ì— GPU ì‚¬ìš©
)

output = model.create_completion("ë‹¹ê·¼íŒŒì¼ëŸ¿ì— ëŒ€í•´ ì•Œë ¤ì¤˜", max_tokens=512)
print(output["choices"][0]["text"])
```
""")
        else:
            print("âŒ íŒŒì¸íŠœë‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ëŒ€ì‹  Hugging Face ëª¨ë¸ì„ ì‚¬ìš©í•´ë³´ì„¸ìš”.")
    else:
        # Hugging Face ëª¨ë¸ ì‚¬ìš©
        print("ğŸ”„ Hugging Face ëª¨ë¸ ì‚¬ìš© ì¤€ë¹„ ì¤‘...")
        
        # ë°ì´í„°ì…‹ HF í¬ë§·ìœ¼ë¡œ ë³€í™˜
        dataset = []
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line.strip())
                dataset.append(data)
        
        formatted_data = []
        for item in dataset:
            formatted_data.append({
                "text": f"<start_of_turn>user\n{item['prompt']}<end_of_turn>\n<start_of_turn>model\n{item['response']}<end_of_turn>"
            })
        
        # HF Datasetìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(formatted_data)
        hf_dataset = Dataset.from_pandas(df)
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ğŸ”„ Gemma 3 í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-4b-instruct", use_fast=True)
        tokenizer.pad_token = tokenizer.eos_token
        
        # ë°ì´í„°ì…‹ í† í°í™”
        def tokenize_function(examples):
            return tokenizer(
                examples["text"],
                padding="max_length",
                truncation=True,
                max_length=1024,
                return_tensors="pt"
            )
        
        tokenized_dataset = hf_dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"]
        )
        
        # í•™ìŠµ/ê²€ì¦ ë¶„í• 
        split_dataset = tokenized_dataset.train_test_split(test_size=0.1)
        
        # 4ë¹„íŠ¸ ì–‘ìí™” ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )
        
        # ëª¨ë¸ ë¡œë“œ
        print("ğŸ”„ Gemma 3 ëª¨ë¸ ë¡œë“œ ì¤‘...")
        try:
            model = AutoModelForCausalLM.from_pretrained(
                "google/gemma-3-4b-instruct",
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True
            )
            
            # LoRA í•™ìŠµì„ ìœ„í•œ ëª¨ë¸ ì¤€ë¹„
            model = prepare_model_for_kbit_training(model)
            
            # LoRA ì„¤ì • - GTX 1660ì— ë§ê²Œ ë§¤ê°œë³€ìˆ˜ ì¶•ì†Œ
            peft_config = LoraConfig(
                task_type="CAUSAL_LM",
                r=8,
                lora_alpha=16,
                lora_dropout=0.05,
                target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
                bias="none",
            )
            
            # LoRA ì–´ëŒ‘í„° ì ìš©
            model = get_peft_model(model, peft_config)
            model.print_trainable_parameters()
            
            # í•™ìŠµ ì¸ì ì„¤ì • - GTX 1660ì— ë§ê²Œ ì¡°ì •
            training_args = TrainingArguments(
                output_dir=output_dir,
                num_train_epochs=1,
                per_device_train_batch_size=2,
                per_device_eval_batch_size=2,
                gradient_accumulation_steps=8,
                evaluation_strategy="steps",
                eval_steps=100,
                save_strategy="steps",
                save_steps=100,
                save_total_limit=2,
                logging_dir=os.path.join(output_dir, "logs"),
                logging_steps=10,
                learning_rate=1e-4,
                weight_decay=0.01,
                fp16=True,
                bf16=False,
                max_grad_norm=0.3,
                warmup_ratio=0.03,
                group_by_length=True,
                lr_scheduler_type="cosine",
                report_to="none",
                optim="paged_adamw_8bit"
            )
            
            # ë°ì´í„° ì½œë ˆì´í„° ì •ì˜
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=tokenizer, 
                mlm=False
            )
            
            # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=split_dataset["train"],
                eval_dataset=split_dataset["test"],
                data_collator=data_collator,
                tokenizer=tokenizer
            )
            
            # ëª¨ë¸ í•™ìŠµ
            print("ğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘...")
            trainer.train()
            
            # ëª¨ë¸ ì €ì¥
            print(f"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ ì¤‘...")
            model.save_pretrained(os.path.join(output_dir, "final_model"))
            tokenizer.save_pretrained(os.path.join(output_dir, "final_model"))
            
            print(f"""
âœ¨ ëª¨ë“  ê³¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!
ğŸ” íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ìœ„ì¹˜: {os.path.join(output_dir, 'final_model')}

íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ:
from transformers import AutoModelForCausalLM, AutoTokenizer

# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("{os.path.join(output_dir, 'final_model')}")
model = AutoModelForCausalLM.from_pretrained("{os.path.join(output_dir, 'final_model')}")

# ì¶”ë¡  ìˆ˜í–‰
prompt = "ë‹¹ê·¼íŒŒì¼ëŸ¿ì— ëŒ€í•´ ì•Œë ¤ì¤˜"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=1024)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
""")
        except Exception as e:
            print(f"âŒ íŒŒì¸íŠœë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            print("ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜ë©´ ë°°ì¹˜ í¬ê¸°, ì—í¬í¬ ìˆ˜, ì»¨í…ìŠ¤íŠ¸ í¬ê¸°ë¥¼ ë” ì¤„ì—¬ë³´ì„¸ìš”.")

if __name__ == "__main__":
    main() 