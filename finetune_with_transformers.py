import os
import json
import argparse
from pathlib import Path
import numpy as np
import torch
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training

def load_dataset(jsonl_path):
    """JSONL íŒŒì¼ì—ì„œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    dataset = []
    try:
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line.strip())
                dataset.append(data)
        print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset)}ê°œ QA ìŒ")
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        if dataset:
            print("\në°ì´í„°ì…‹ ìƒ˜í”Œ:")
            print(f"ì§ˆë¬¸: {dataset[0]['prompt'][:100]}...")
            print(f"ë‹µë³€: {dataset[0]['response'][:100]}...")
        
        return dataset
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None

def format_dataset(dataset, model_type="llama"):
    """íŒŒì¸íŠœë‹ì„ ìœ„í•œ í¬ë§·ìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ë³€í™˜í•©ë‹ˆë‹¤."""
    formatted_data = []
    
    if model_type.lower() in ["llama", "mistral"]:
        # Llama/Mistral ëª¨ë¸ì„ ìœ„í•œ í¬ë§·
        for item in dataset:
            formatted_data.append({
                "text": f"<s>[INST] {item['prompt']} [/INST] {item['response']}</s>"
            })
    elif model_type.lower() == "gemma":
        # Gemma ëª¨ë¸ì„ ìœ„í•œ í¬ë§·
        for item in dataset:
            formatted_data.append({
                "text": f"<start_of_turn>user\n{item['prompt']}<end_of_turn>\n<start_of_turn>model\n{item['response']}<end_of_turn>"
            })
    else:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")
        return None
    
    # HF Datasetìœ¼ë¡œ ë³€í™˜
    try:
        hf_dataset = Dataset.from_pandas(pd.DataFrame(formatted_data))
        print(f"âœ… HF Dataset ë³€í™˜ ì™„ë£Œ: {len(hf_dataset)}ê°œ í•­ëª©")
        return hf_dataset
    except Exception as e:
        print(f"âŒ HF Dataset ë³€í™˜ ì‹¤íŒ¨: {str(e)}")
        return None

def tokenize_data(dataset, tokenizer, max_length=2048):
    """ë°ì´í„°ì…‹ì„ í† í°í™”í•©ë‹ˆë‹¤."""
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=max_length,
            return_tensors="pt"
        )
    
    try:
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"]
        )
        print(f"âœ… ë°ì´í„°ì…‹ í† í°í™” ì™„ë£Œ")
        return tokenized_dataset
    except Exception as e:
        print(f"âŒ ë°ì´í„°ì…‹ í† í°í™” ì‹¤íŒ¨: {str(e)}")
        return None

def train_model(model_name, dataset_path, output_dir, epochs=3, batch_size=8, model_type="llama", quantize=False):
    """Transformersë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒì¸íŠœë‹ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        qa_dataset = load_dataset(dataset_path)
        if not qa_dataset:
            return False
        
        # ë°ì´í„°ì…‹ í¬ë§·íŒ…
        formatted_dataset = format_dataset(qa_dataset, model_type)
        if formatted_dataset is None:
            return False
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        print(f"ğŸ”„ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘: {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ë°ì´í„°ì…‹ í† í°í™”
        tokenized_dataset = tokenize_data(formatted_dataset, tokenizer)
        if tokenized_dataset is None:
            return False
        
        # í•™ìŠµ/ê²€ì¦ ë¶„í• 
        split_dataset = tokenized_dataset.train_test_split(test_size=0.1)
        
        # ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        
        # ì–‘ìí™” ì—¬ë¶€ì— ë”°ë¼ ëª¨ë¸ ë¡œë“œ ë°©ì‹ ë‹¤ë¥´ê²Œ ì„¤ì •
        if quantize:
            from transformers import BitsAndBytesConfig
            
            # BitsAndBytes ì„¤ì •
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16
            )
            
            # ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto"
            )
            
            # ì–‘ìí™”ëœ ëª¨ë¸ì„ LoRA í•™ìŠµì„ ìœ„í•´ ì¤€ë¹„
            model = prepare_model_for_kbit_training(model)
        else:
            # ì¼ë°˜ ëª¨ë¸ ë¡œë“œ
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
        
        # LoRA ì„¤ì •
        peft_config = LoraConfig(
            task_type="CAUSAL_LM",
            r=16,
            lora_alpha=32,
            lora_dropout=0.05,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
            bias="none",
        )
        
        # LoRA ì–´ëŒ‘í„° ì ìš©
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
        
        # í•™ìŠµ ì¸ì ì„¤ì •
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=4,
            evaluation_strategy="steps",
            eval_steps=50,
            save_strategy="steps",
            save_steps=50,
            save_total_limit=3,
            logging_dir=os.path.join(output_dir, "logs"),
            logging_steps=10,
            learning_rate=2e-4,
            weight_decay=0.01,
            fp16=True,
            bf16=False,
            max_grad_norm=0.3,
            warmup_ratio=0.03,
            group_by_length=True,
            lr_scheduler_type="cosine",
            report_to="none"
        )
        
        # ë°ì´í„° ì½œë ˆì´í„° ì •ì˜
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer, 
            mlm=False
        )
        
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=split_dataset["train"],
            eval_dataset=split_dataset["test"],
            data_collator=data_collator,
            tokenizer=tokenizer
        )
        
        # ëª¨ë¸ í•™ìŠµ
        print("ğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘...")
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        print(f"âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ ì¤‘...")
        model.save_pretrained(os.path.join(output_dir, "final_model"))
        tokenizer.save_pretrained(os.path.join(output_dir, "final_model"))
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {os.path.join(output_dir, 'final_model')}")
        return True
        
    except Exception as e:
        print(f"âŒ íŒŒì¸íŠœë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

def main():
    parser = argparse.ArgumentParser(description="Transformersë¥¼ ì‚¬ìš©í•œ ë‹¹ê·¼íŒŒì¼ëŸ¿ ë°ì´í„° íŒŒì¸íŠœë‹")
    parser.add_argument("--model", required=True, help="Hugging Face ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ")
    parser.add_argument("--model-type", default="llama", choices=["llama", "mistral", "gemma"], 
                        help="ëª¨ë¸ ìœ í˜• (llama, mistral, gemma)")
    parser.add_argument("--dataset", default="carrotpilot_data_with_images/carrotpilot_finetuning_dataset.jsonl", 
                        help="JSONL ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output-dir", default="carrotpilot_finetuned_hf", help="íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬")
    parser.add_argument("--epochs", type=int, default=3, help="ì—í¬í¬ ìˆ˜")
    parser.add_argument("--batch-size", type=int, default=8, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--quantize", action="store_true", help="4ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€")
    
    args = parser.parse_args()
    
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ import ì²´í¬
    try:
        import transformers
        import datasets
        import peft
        import pandas as pd
        print("âœ… í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ëª¨ë‘ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    except ImportError as e:
        print(f"âŒ í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤: {e}")
        print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install transformers datasets peft pandas")
        return
    
    if args.quantize:
        try:
            import bitsandbytes
            print("âœ… bitsandbytesê°€ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        except ImportError:
            print("âŒ ì–‘ìí™”ë¥¼ ìœ„í•œ bitsandbytesê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install bitsandbytes")
            print("ê³„ì† ì§„í–‰í•˜ë ¤ë©´ --quantize ì˜µì…˜ì„ ì œê±°í•˜ê±°ë‚˜ bitsandbytesë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
            return
    
    # GPU í™•ì¸
    if torch.cuda.is_available():
        print(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
    else:
        print("âš ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤. ì´ëŠ” ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ë°ì´í„°ì…‹ í™•ì¸
    if not os.path.exists(args.dataset):
        print(f"âŒ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.dataset}")
        return
    
    # íŒŒì¸íŠœë‹ ì‹¤í–‰
    success = train_model(
        args.model, 
        args.dataset, 
        args.output_dir, 
        args.epochs, 
        args.batch_size, 
        args.model_type,
        args.quantize
    )
    
    if success:
        print("âœ¨ ëª¨ë“  ê³¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ” íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ìœ„ì¹˜: {os.path.join(args.output_dir, 'final_model')}")
        print(f"""
íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ:
from transformers import AutoModelForCausalLM, AutoTokenizer

# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("{os.path.join(args.output_dir, 'final_model')}")
model = AutoModelForCausalLM.from_pretrained("{os.path.join(args.output_dir, 'final_model')}")

# ì¶”ë¡  ìˆ˜í–‰
prompt = "ë‹¹ê·¼íŒŒì¼ëŸ¿ì— ëŒ€í•´ ì•Œë ¤ì¤˜"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=1024)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
""")
    else:
        print("âŒ íŒŒì¸íŠœë‹ ê³¼ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 